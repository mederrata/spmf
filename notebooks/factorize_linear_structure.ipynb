{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import arviz as az\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "sys.path.append('../')\n",
    "from mederrata_spmf import PoissonMatrixFactorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we look at the $\\mathcal{M}$-open setting, where the generating process is in the span of models.\n",
    "\n",
    "# Generate a random matrices V, W\n",
    "\n",
    "For V, assume that 10 variables share a factor structure and the other 20 are noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50000\n",
    "D_factor = 10\n",
    "D_noise = 20\n",
    "D = D_factor + D_noise\n",
    "P = 3\n",
    "\n",
    "V = np.abs(np.random.normal(1.5, 0.5, size=(P,D_factor)))\n",
    "Z = np.abs(np.random.normal(0, 1, size=(N,P)))\n",
    "\n",
    "ZV = Z.dot(V)\n",
    "\n",
    "X = np.zeros((N, D_factor+D_noise))\n",
    "X = np.random.poisson(1.,size=(N,D_noise+D_factor))\n",
    "X[:, ::3] = np.random.poisson(ZV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test taking in from tf.dataset, don't pre-batch\n",
    "data = tf.data.Dataset.from_tensor_slices(\n",
    "    {\n",
    "        'data': X,\n",
    "        'indices': np.arange(N),\n",
    "        'normalization': np.ones(N)\n",
    "    })\n",
    "\n",
    "data = data.batch(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dim: 30 -> Latent dim 3\n"
     ]
    }
   ],
   "source": [
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "strategy = None\n",
    "factor = PoissonMatrixFactorization(\n",
    "    data, latent_dim=P, strategy=strategy,\n",
    "    scale_rates=True, with_s=True,\n",
    "    u_tau_scale=1.0/np.sqrt(D*N),\n",
    "    dtype=tf.float64)\n",
    "# Test to make sure sampling works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0604 09:33:58.254903 4603870656 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 57.73558764066866\n",
      "Epoch 1: average-batch loss: 53.81824359627665 last batch loss: 53.28997756997598\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-1\n",
      "Epoch 2: average-batch loss: 52.54408918387699 last batch loss: 52.234162657365125\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-2\n",
      "Epoch 3: average-batch loss: 51.61245086404765 last batch loss: 51.375159926501574\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-3\n",
      "Epoch 4: average-batch loss: 50.87532751947661 last batch loss: 50.73142827765426\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-4\n",
      "Epoch 5: average-batch loss: 50.5296842536969 last batch loss: 50.535977429079466\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-5\n",
      "Epoch 6: average-batch loss: 49.96354488647245 last batch loss: 49.470640554144744\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-6\n",
      "Epoch 7: average-batch loss: 48.451906260999785 last batch loss: 47.945292393791874\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-7\n",
      "Epoch 8: average-batch loss: 47.42285915811596 last batch loss: 47.34844602833972\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-8\n",
      "Epoch 9: average-batch loss: 46.952554825074934 last batch loss: 47.11391520079796\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-9\n",
      "Epoch 10: average-batch loss: 46.80342402077337 last batch loss: 46.98688972380689\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-10\n",
      "Epoch 11: average-batch loss: 46.709596527289385 last batch loss: 46.87431536270738\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-11\n",
      "Epoch 12: average-batch loss: 46.66009333692012 last batch loss: 46.88237063890748\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-12\n",
      "Epoch 13: average-batch loss: 46.58184670847514 last batch loss: 46.75641971805277\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-13\n",
      "Epoch 14: average-batch loss: 46.530020428188784 last batch loss: 46.76743910509923\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-14\n",
      "Epoch 15: average-batch loss: 46.50741015325493 last batch loss: 46.714371665602144\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-15\n",
      "Epoch 16: average-batch loss: 46.50796657569237 last batch loss: 46.6955907465717\n",
      "Epoch 17: average-batch loss: 46.48887980798167 last batch loss: 46.68840078630551\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-16\n",
      "Epoch 18: average-batch loss: 46.474518041307185 last batch loss: 46.690005454466494\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-17\n",
      "Epoch 19: average-batch loss: 46.46499327100861 last batch loss: 46.682551349827456\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-18\n",
      "Epoch 20: average-batch loss: 46.47789871938253 last batch loss: 46.69540518401768\n",
      "We are in a loss plateau learning rate: 0.09000000000000001 loss: 46.42119002532144\n",
      "Restoring from a checkpoint - loss: 46.446721520798874\n",
      "Epoch 21: average-batch loss: 46.46184644869657 last batch loss: 46.71283488889018\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-19\n",
      "Epoch 22: average-batch loss: 46.45729061995417 last batch loss: 46.68813481533183\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-20\n",
      "Epoch 23: average-batch loss: 46.42989866194411 last batch loss: 46.63505319049486\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-21\n",
      "Epoch 24: average-batch loss: 46.4172982104115 last batch loss: 46.63774945873936\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-22\n",
      "Epoch 25: average-batch loss: 46.41526197417437 last batch loss: 46.618063026577694\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-23\n",
      "Epoch 26: average-batch loss: 46.40557230668744 last batch loss: 46.57575642067358\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-24\n",
      "Epoch 27: average-batch loss: 46.383667850124766 last batch loss: 46.58994226043483\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-25\n",
      "Epoch 28: average-batch loss: 46.39449462397738 last batch loss: 46.53924854144574\n",
      "Epoch 29: average-batch loss: 46.38151071233395 last batch loss: 46.56809951706663\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-26\n",
      "Epoch 30: average-batch loss: 46.373418342555354 last batch loss: 46.57259028271629\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-27\n",
      "Epoch 31: average-batch loss: 46.374676141429724 last batch loss: 46.57040118493705\n",
      "Epoch 32: average-batch loss: 46.373126609239826 last batch loss: 46.5714581248337\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-28\n",
      "Epoch 33: average-batch loss: 46.364137662174336 last batch loss: 46.56937228702783\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-29\n",
      "Epoch 34: average-batch loss: 46.355695321265 last batch loss: 46.536182777630934\n",
      "Saved a checkpoint: ./.tf_ckpts/chkpt-30\n",
      "Epoch 35: average-batch loss: 46.36492446751175 last batch loss: 46.554104284092546\n",
      "We are in a loss plateau learning rate: 0.08100000000000002 loss: 46.34278225704986\n",
      "Restoring from a checkpoint - loss: 46.34230597305995\n",
      "Epoch 36: average-batch loss: 46.368881569432645 last batch loss: 46.58770402169743\n",
      "Epoch 37: average-batch loss: 46.374030093296206 last batch loss: 46.55864535228316\n",
      "Epoch 38: average-batch loss: 46.36783936008797 last batch loss: 46.5576102708914\n",
      "Epoch 39: average-batch loss: 46.3649676252988 last batch loss: 46.61676650631947\n",
      "Epoch 40: average-batch loss: 46.359881884056456 last batch loss: 46.5502061697076\n",
      "Epoch 41: average-batch loss: 46.36316938122419 last batch loss: 46.56065256413553\n",
      "Epoch 42: average-batch loss: 46.37142769195453 last batch loss: 46.57761083411823\n",
      "We are in a loss plateau learning rate: 0.0729 loss: 46.309156982993244\n",
      "Restoring from a checkpoint - loss: 46.28287929940957\n",
      "Epoch 43: average-batch loss: 46.357538516709134 last batch loss: 46.58457341891341\n",
      "Epoch 44: average-batch loss: 46.37470176778317 last batch loss: 46.55932873680669\n",
      "Epoch 45: average-batch loss: 46.361155466324114 last batch loss: 46.58313113979794\n",
      "Epoch 46: average-batch loss: 46.36715562816179 last batch loss: 46.544047311967645\n"
     ]
    }
   ],
   "source": [
    "losses = factor.calibrate_advi(\n",
    "    num_epochs=200, learning_rate=.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waic = factor.waic()\n",
    "print(waic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "surrogate_samples = factor.surrogate_distribution.sample(1000)\n",
    "if 's' in surrogate_samples.keys():\n",
    "    weights = surrogate_samples['s']/tf.reduce_sum(surrogate_samples['s'],-2,keepdims=True)\n",
    "    intercept_data = az.convert_to_inference_data(\n",
    "        {\n",
    "            r\"$w_d$\": \n",
    "                (tf.squeeze(surrogate_samples['w'])*weights[:,-1,:]*factor.column_norm_factor).numpy().T})\n",
    "else:\n",
    "    intercept_data = az.convert_to_inference_data(\n",
    "    {\n",
    "        r\"$w_d$\": \n",
    "            (tf.squeeze(surrogate_samples['w'])*factor.column_norm_factor).numpy().T})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(14,8))\n",
    "D = factor.feature_dim\n",
    "pcm = ax[0].imshow(factor.encoding_matrix().numpy()[::-1,:], vmin=0, cmap=\"Blues\")\n",
    "ax[0].set_yticks(np.arange(D))\n",
    "ax[0].set_yticklabels(np.arange(D))\n",
    "ax[0].set_ylabel(\"item\")\n",
    "ax[0].set_xlabel(\"factor dimension\")\n",
    "ax[0].set_xticks(np.arange(P))\n",
    "ax[0].set_xticklabels(np.arange(P))\n",
    "\n",
    "fig.colorbar(pcm, ax=ax[0], orientation = \"vertical\")\n",
    "az.plot_forest(intercept_data, ax=ax[1])\n",
    "ax[1].set_xlabel(\"background rate\")\n",
    "ax[1].set_ylim((-0.014,.466))\n",
    "ax[1].set_title(\"65% and 95% CI\")\n",
    "ax[1].axvline(1.0, linestyle='dashed', color=\"black\")\n",
    "#plt.savefig('mix_factorization_sepmf.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(factor.decoding_matrix().numpy().dot(factor.encoding_matrix().numpy()),cmap=\"Greens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bitf92b1a92eaa047aebe1f2ca9ebe04d21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
